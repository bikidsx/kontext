# Kontext

## Ultra-Fast Contextual Memory for AI Agents

Kontext is a high-performance memory framework designed for AI agents operating in domain-specific environments. Built on FalkorDB's unified graph-vector architecture, Kontext provides sub-50ms context retrieval while maintaining rich semantic and structural relationships.

### The Problem

Modern AI agents suffer from **context amnesia**—they lack persistent memory across interactions, leading to:

- Repetitive questioning of users
- Inability to learn preferences over time
- Loss of critical domain knowledge between sessions
- Inconsistent behavior across conversations

### The Solution

Kontext provides a **unified memory layer** that combines:

| Capability | Traditional Approach | Kontext |
|------------|---------------------|---------|
| Semantic Search | Separate Vector DB | Native in FalkorDB |
| Relationship Tracking | Separate Graph DB | Native in FalkorDB |
| Full-Text Search | Elasticsearch | Native in FalkorDB |
| Temporal Reasoning | Custom Logic | Built-in |
| **Total Latency** | 50-200ms | **< 30ms** |

### Key Features

- **Ultra-Fast**: Sub-30ms context retrieval via FalkorDB
- **Automatic Extraction**: LLM-powered entity and relationship extraction
- **Temporal Awareness**: Track when facts become true or false
- **Simple API**: Three methods: `add()`, `search()`, `getContext()`
- **Domain-Optimized**: Pre-configured for hotel management (extensible)
- **Async Processing**: Fire-and-forget memory storage

```typescript
import { Kontext } from 'kontext';

const kontext = new Kontext({
  llm: { provider: 'gemini', model: 'gemini-2.5-flash' }
});

// Add conversation to memory
await kontext.add([
  { role: 'user', content: 'I prefer ocean-view rooms' }
], { userId: 'guest-123' });

// Retrieve context for agent
const context = await kontext.getContext(
  'Book a room for this guest',
  { userId: 'guest-123' }
);
// → "Guest prefers ocean-view rooms"
```

### Why FalkorDB?

Kontext is built exclusively on FalkorDB because it's the only database that provides:

1. **Graph + Vector + Full-Text in One Query**
2. **Sub-millisecond Latency** (in-memory architecture)
3. **Redis Protocol Compatibility** (familiar tooling)
4. **Single Container Deployment** (no infrastructure complexity)

This eliminates the need for separate Pinecone + Neo4j + Elasticsearch deployments.
